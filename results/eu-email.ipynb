{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a1565-3abe-4bf6-babb-f1e4edad839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph from: D:\\pbl\\Datasets\\email-Eu-core.txt\n",
      "Loading ground truth from: D:\\pbl\\Datasets\\email-Eu-core-department-labels.txt\n",
      "Graph: email-Eu-core\n",
      "Nodes: 1005, Edges: 16706\n",
      "\n",
      "--- Running OBPSO-AIW with Full Evaluation ---\n",
      "Iteration 1/100, Best Modularity: 0.4323\n",
      "Iteration 51/100, Best Modularity: 0.4323\n",
      "\n",
      "--- ✅ Final Results for email-Eu-core ---\n",
      "Modularity (Q)           : 0.4324\n",
      "Modularity Density (QDS) : 0.5569\n",
      "Conductance              : 0.1375\n",
      "Execution Time (s)       : 456.3414\n",
      "Memory Usage (MB)        : 5.2227\n",
      "Number of Communities    : 28\n",
      "NMI                      : 0.5940\n",
      "ARI                      : 0.3297\n",
      "Macro F1 Score           : 0.2456\n",
      "\n",
      "--- Starting Stability Calculation (this may take a while) ---\n",
      "\n",
      "--- Calculating Stability under Perturbation ---\n",
      "Iteration 1/50, Best Modularity: 0.4323\n",
      "Iteration 1/50, Best Modularity: 0.4334\n",
      "Perturbation Run 1/3, NMI vs Original: 0.8994\n",
      "Iteration 1/50, Best Modularity: 0.4333\n",
      "Perturbation Run 2/3, NMI vs Original: 0.9045\n",
      "Iteration 1/50, Best Modularity: 0.4331\n",
      "Perturbation Run 3/3, NMI vs Original: 0.9127\n",
      "--- Stability Calculation Finished. Average NMI: 0.9055 ---\n",
      "Stability (Avg. NMI)     : 0.9055\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from numpy.random import rand, randint\n",
    "from sklearn.metrics import normalized_mutual_info_score as nmi\n",
    "from sklearn.metrics import adjusted_rand_score as ari\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from networkx.algorithms import community\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: NEW METRIC CALCULATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def calculate_performance_metrics():\n",
    "    \"\"\"Calculates current memory usage of the process.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    memory_usage = mem_info.rss / (1024 * 1024)  # Convert bytes to MB\n",
    "    return memory_usage\n",
    "\n",
    "def calculate_conductance(G, communities):\n",
    "    \"\"\"Calculates the average conductance of the partition.\"\"\"\n",
    "    if not communities:\n",
    "        return 0.0\n",
    "    \n",
    "    conductance_scores = [nx.conductance(G, comm) for comm in communities if len(comm) > 0 and G.subgraph(comm).number_of_edges() > 0]\n",
    "    return np.mean(conductance_scores) if conductance_scores else 0.0\n",
    "\n",
    "\n",
    "def calculate_qds(G, communities):\n",
    "    \"\"\"Calculates Modularity Density (Qds) for a partition.\"\"\"\n",
    "    m = G.number_of_edges()\n",
    "    if m == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    qds_val = 0.0\n",
    "    for community_nodes in communities:\n",
    "        if not community_nodes:\n",
    "            continue\n",
    "        \n",
    "        # Create a set for faster lookups\n",
    "        community_set = set(community_nodes)\n",
    "        \n",
    "        # Calculate internal and boundary degrees\n",
    "        internal_degree = 0\n",
    "        boundary_degree = 0\n",
    "        \n",
    "        for node in community_nodes:\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if neighbor in community_set:\n",
    "                    internal_degree += 1\n",
    "                else:\n",
    "                    boundary_degree += 1\n",
    "                    \n",
    "        mc = internal_degree / 2  # Each internal edge counted twice\n",
    "        nc = len(community_nodes)\n",
    "        dc = internal_degree + boundary_degree\n",
    "        \n",
    "        if nc > 1:\n",
    "            internal_density = (2 * mc) / (nc * (nc - 1))\n",
    "            qds_val += (mc / m) - (dc / (2 * m))**2 * internal_density\n",
    "            \n",
    "    return qds_val\n",
    "\n",
    "def calculate_macro_f1(true_labels_dict, pred_labels_list):\n",
    "    \"\"\"\n",
    "    Calculates the Macro F1 Score between true and predicted communities.\n",
    "    This is a common approach for comparing clustering results.\n",
    "    \"\"\"\n",
    "    # Convert predicted list of lists to a dictionary for easy lookup\n",
    "    pred_labels_dict = {node: i for i, comm in enumerate(pred_labels_list) for node in comm}\n",
    "    \n",
    "    nodes = list(true_labels_dict.keys())\n",
    "    true_labels = [true_labels_dict[node] for node in nodes]\n",
    "    pred_labels = [pred_labels_dict.get(node, -1) for node in nodes]\n",
    "\n",
    "    cm = contingency_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    f1_scores = []\n",
    "    # Calculate F1 for each true community (row in contingency matrix)\n",
    "    for i in range(cm.shape[0]):\n",
    "        true_community_size = np.sum(cm[i, :])\n",
    "        \n",
    "        if true_community_size == 0:\n",
    "            continue\n",
    "\n",
    "        best_match_j = np.argmax(cm[i, :])\n",
    "        best_match_size = cm[i, best_match_j]\n",
    "        predicted_community_size = np.sum(cm[:, best_match_j])\n",
    "        \n",
    "        precision = best_match_size / predicted_community_size if predicted_community_size > 0 else 0\n",
    "        recall = best_match_size / true_community_size\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "            f1_scores.append(f1)\n",
    "    \n",
    "    return np.mean(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "def calculate_stability(graph, algorithm_class, runs=10, perturbation_rate=0.05, **kwargs):\n",
    "    \"\"\"Calculates the stability of the algorithm under edge perturbation.\"\"\"\n",
    "    print(\"\\n--- Calculating Stability under Perturbation ---\")\n",
    "    \n",
    "    # Get the baseline result on the original graph\n",
    "    algo_instance = algorithm_class(graph, **kwargs)\n",
    "    base_partition_labels, _ = algo_instance.run() # We only need the labels\n",
    "    \n",
    "    nmi_scores = []\n",
    "    num_edges_to_remove = int(graph.number_of_edges() * perturbation_rate)\n",
    "\n",
    "    for i in range(runs):\n",
    "        perturbed_graph = graph.copy()\n",
    "        \n",
    "        # Randomly select and remove edges\n",
    "        if num_edges_to_remove > 0:\n",
    "            edges_to_remove_indices = np.random.choice(len(perturbed_graph.edges()), num_edges_to_remove, replace=False)\n",
    "            edges = list(perturbed_graph.edges())\n",
    "            perturbed_graph.remove_edges_from([edges[i] for i in edges_to_remove_indices])\n",
    "        \n",
    "        perturbed_algo = algorithm_class(perturbed_graph, **kwargs)\n",
    "        perturbed_labels, _ = perturbed_algo.run()\n",
    "        \n",
    "        score = nmi(base_partition_labels, perturbed_labels)\n",
    "        nmi_scores.append(score)\n",
    "        print(f\"Perturbation Run {i+1}/{runs}, NMI vs Original: {score:.4f}\")\n",
    "        \n",
    "    stability_score = np.mean(nmi_scores)\n",
    "    print(f\"--- Stability Calculation Finished. Average NMI: {stability_score:.4f} ---\")\n",
    "    return stability_score\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: ALGORITHM CLASSES\n",
    "# ==============================================================================\n",
    "\n",
    "class Standard_PSO:\n",
    "    def __init__(self, graph, population_size=200, max_iter=500):\n",
    "        self.graph = graph\n",
    "        self.n = len(graph.nodes)\n",
    "        self.population_size = population_size\n",
    "        self.max_iter = max_iter\n",
    "        self.omega = 0.7\n",
    "        self.c1 = 1.5\n",
    "        self.c2 = 1.5\n",
    "        self.population = self.initialize_population()\n",
    "\n",
    "    def initialize_population(self):\n",
    "        return np.array([randint(0, self.n, self.n) for _ in range(self.population_size)])\n",
    "\n",
    "    def calculate_modularity(self, community_assignment):\n",
    "        return nx.community.modularity(self.graph, self.get_communities(community_assignment))\n",
    "\n",
    "    def get_communities(self, community_assignment):\n",
    "        communities = {}\n",
    "        for node, com in enumerate(community_assignment):\n",
    "            communities.setdefault(com, []).append(node)\n",
    "        return list(communities.values())\n",
    "\n",
    "    def run(self):\n",
    "        velocities = np.zeros((self.population_size, self.n))\n",
    "        personal_best = self.population.copy()\n",
    "        personal_best_scores = np.array([self.calculate_modularity(ind) for ind in self.population])\n",
    "        global_best = personal_best[np.argmax(personal_best_scores)]\n",
    "        global_best_score = max(personal_best_scores)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            r1, r2 = rand(self.population_size, self.n), rand(self.population_size, self.n)\n",
    "            velocities = (self.omega * velocities + self.c1 * r1 * (personal_best - self.population) \n",
    "                          + self.c2 * r2 * (global_best - self.population))\n",
    "            self.population = np.round(self.population + velocities).astype(int) % self.n\n",
    "            self.population = np.clip(self.population, 0, self.n - 1)\n",
    "\n",
    "            fitness = np.array([self.calculate_modularity(ind) for ind in self.population])\n",
    "            update_mask = fitness > personal_best_scores\n",
    "            personal_best[update_mask] = self.population[update_mask]\n",
    "            personal_best_scores[update_mask] = fitness[update_mask]\n",
    "\n",
    "            if max(fitness) > global_best_score:\n",
    "                global_best_score = max(fitness)\n",
    "                global_best = self.population[np.argmax(fitness)]\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print(f\"Iteration {iteration+1}/{self.max_iter}, Best Modularity: {global_best_score}\")\n",
    "        \n",
    "        return global_best\n",
    "\n",
    "\n",
    "class OBPSO_AIW:\n",
    "    def __init__(self, graph, population_size=150, max_iter=500, ground_truth=None):\n",
    "        self.graph = graph\n",
    "        self.n = len(graph.nodes)\n",
    "        self.population_size = population_size\n",
    "        self.max_iter = max_iter\n",
    "        self.omega_max = 0.9\n",
    "        self.omega_min = 0.2 # Adjusted for better convergence\n",
    "        self.c1 = 1.2\n",
    "        self.c2 = 1.8\n",
    "        self.mutation_rate = 0.25\n",
    "        self.crossover_rate = 0.9\n",
    "        self.population = self.initialize_population()\n",
    "        self.ground_truth = ground_truth # Store ground truth if provided\n",
    "\n",
    "    # In the OBPSO_AIW class\n",
    "\n",
    "    def initialize_population(self):\n",
    "        half_pop = self.population_size // 2\n",
    "        \n",
    "        # --- Part 1: Louvain Initialization ---\n",
    "        initial_partition = community.louvain_communities(self.graph, seed=42)\n",
    "        community_map = {node: i for i, comm in enumerate(initial_partition) for node in comm}\n",
    "        nodes = list(self.graph.nodes())\n",
    "        base_assignment = np.array([community_map[node] for node in nodes])\n",
    "        louvain_population = [base_assignment.copy() for _ in range(half_pop)]\n",
    "        \n",
    "        # --- Part 2: Random Initialization ---\n",
    "        random_population = [randint(0, self.n, self.n) for _ in range(self.population_size - half_pop)]\n",
    "        \n",
    "        # --- Combine them ---\n",
    "        full_population = louvain_population + random_population\n",
    "        return np.array(full_population)\n",
    "\n",
    "    def calculate_modularity(self, community_assignment):\n",
    "        return nx.community.modularity(self.graph, self.get_communities(community_assignment))\n",
    "\n",
    "    def get_communities(self, community_assignment):\n",
    "        communities = defaultdict(list)\n",
    "        # Ensure we map based on the actual node IDs from the graph\n",
    "        for i, com_id in enumerate(community_assignment):\n",
    "            communities[com_id].append(list(self.graph.nodes())[i])\n",
    "        return list(communities.values())\n",
    "\n",
    "    def crossover(self, population):\n",
    "        new_population = []\n",
    "        for i in range(0, self.population_size, 2):\n",
    "            parent1 = population[i]\n",
    "            if i + 1 < self.population_size:\n",
    "                parent2 = population[i+1]\n",
    "                if rand() < self.crossover_rate:\n",
    "                    # Single point crossover\n",
    "                    crossover_point = randint(1, self.n - 1)\n",
    "                    child1 = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n",
    "                    child2 = np.concatenate([parent2[:crossover_point], parent1[crossover_point:]])\n",
    "                    new_population.extend([child1, child2])\n",
    "                else:\n",
    "                    new_population.extend([parent1, parent2])\n",
    "            else:\n",
    "                new_population.append(parent1)\n",
    "        return np.array(new_population)\n",
    "\n",
    "    # In the OBPSO_AIW class\n",
    "\n",
    "    def mutate(self, population, iteration):\n",
    "        dynamic_mutation = self.mutation_rate * (1 - iteration / self.max_iter)\n",
    "        for individual in population:\n",
    "            if rand() < dynamic_mutation:\n",
    "                mutation_point = randint(0, self.n)\n",
    "                \n",
    "                # --- NEW AGGRESSIVE MUTATION ---\n",
    "                # Find the highest existing community ID and add 1 to create a new one.\n",
    "                # This forces the creation of a new, small community.\n",
    "                new_community_id = np.max(individual) + 1\n",
    "                individual[mutation_point] = new_community_id\n",
    "                \n",
    "        return population\n",
    "\n",
    "    def merge_small_communities(self, population):\n",
    "        for i, individual in enumerate(population):\n",
    "            unique_comms, counts = np.unique(individual, return_counts=True)\n",
    "            comm_map = dict(zip(unique_comms, counts))\n",
    "            \n",
    "            new_individual = individual.copy()\n",
    "            for node_idx, comm_id in enumerate(individual):\n",
    "                if comm_map[comm_id] < 2: # Merge communities with fewer than 3 nodes\n",
    "                    neighbors = list(self.graph.neighbors(list(self.graph.nodes())[node_idx]))\n",
    "                    if neighbors:\n",
    "                        # Find the largest community among neighbors\n",
    "                        neighbor_comms = [individual[list(self.graph.nodes()).index(n)] for n in neighbors]\n",
    "                        if neighbor_comms:\n",
    "                            largest_neighbor_comm = max(set(neighbor_comms), key=neighbor_comms.count)\n",
    "                            new_individual[node_idx] = largest_neighbor_comm\n",
    "            population[i] = new_individual\n",
    "        return population\n",
    "\n",
    "    def run(self):\n",
    "        start_time = time.time()\n",
    "        initial_mem = calculate_performance_metrics()\n",
    "        \n",
    "        velocities = np.zeros((self.population_size, self.n))\n",
    "        personal_best = self.population.copy()\n",
    "        personal_best_scores = np.array([self.calculate_modularity(ind) for ind in self.population])\n",
    "        global_best_idx = np.argmax(personal_best_scores)\n",
    "        global_best = personal_best[global_best_idx]\n",
    "        global_best_score = personal_best_scores[global_best_idx]\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            omega = self.omega_max - (self.omega_max - self.omega_min) * (iteration / self.max_iter)\n",
    "            r1, r2 = rand(self.population_size, self.n), rand(self.population_size, self.n)\n",
    "            \n",
    "            velocities = (omega * velocities + \n",
    "                          self.c1 * r1 * (personal_best - self.population) + \n",
    "                          self.c2 * r2 * (global_best - self.population))\n",
    "            \n",
    "            self.population = np.round(self.population + velocities).astype(int)\n",
    "            self.population = np.clip(self.population, 0, self.n - 1)\n",
    "\n",
    "            self.population = self.merge_small_communities(self.population)\n",
    "            self.population = self.mutate(self.population, iteration)\n",
    "            self.population = self.crossover(self.population)\n",
    "\n",
    "            fitness = np.array([self.calculate_modularity(ind) for ind in self.population])\n",
    "            \n",
    "            update_mask = fitness > personal_best_scores\n",
    "            personal_best[update_mask] = self.population[update_mask]\n",
    "            personal_best_scores[update_mask] = fitness[update_mask]\n",
    "\n",
    "            current_best_idx = np.argmax(fitness)\n",
    "            if fitness[current_best_idx] > global_best_score:\n",
    "                global_best_score = fitness[current_best_idx]\n",
    "                global_best = self.population[current_best_idx]\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                print(f\"Iteration {iteration+1}/{self.max_iter}, Best Modularity: {global_best_score:.4f}\")\n",
    "\n",
    "        # --- FINAL METRIC CALCULATION ---\n",
    "        final_mem = calculate_performance_metrics()\n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        final_communities = self.get_communities(global_best)\n",
    "        \n",
    "        results = {\n",
    "            \"Modularity (Q)\": global_best_score,\n",
    "            \"Modularity Density (QDS)\": calculate_qds(self.graph, final_communities),\n",
    "            \"Conductance\": calculate_conductance(self.graph, final_communities),\n",
    "            \"Execution Time (s)\": exec_time,\n",
    "            \"Memory Usage (MB)\": final_mem - initial_mem,\n",
    "            \"Number of Communities\": len(final_communities),\n",
    "        }\n",
    "\n",
    "        if self.ground_truth:\n",
    "            # Align graph nodes for correct label mapping\n",
    "            node_order = list(self.graph.nodes())\n",
    "            true_labels = [self.ground_truth[node] for node in node_order]\n",
    "            \n",
    "            results[\"NMI\"] = nmi(true_labels, global_best)\n",
    "            results[\"ARI\"] = ari(true_labels, global_best)\n",
    "            results[\"Macro F1 Score\"] = calculate_macro_f1(self.ground_truth, final_communities)\n",
    "\n",
    "        return global_best, results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: MAIN EXECUTION BLOCK\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- DATA LOADING FOR EMAIL-EU-CORE ---\n",
    "    # Define the file paths. (Assumes files are in the same folder as the script)\n",
    "    edge_file_path = r\"D:\\pbl\\Datasets\\email-Eu-core.txt\"\n",
    "    label_file_path = r\"D:\\pbl\\Datasets\\email-Eu-core-department-labels.txt\"\n",
    "\n",
    "    print(f\"Loading graph from: {edge_file_path}\")\n",
    "    # The graph nodes are integers, so we specify nodetype=int\n",
    "    G = nx.read_edgelist(edge_file_path, create_using=nx.Graph(), nodetype=int)\n",
    "    \n",
    "    # Load the ground truth communities from the labels file\n",
    "    print(f\"Loading ground truth from: {label_file_path}\")\n",
    "    ground_truth_dict = {}\n",
    "    with open(label_file_path) as f:\n",
    "        for line in f:\n",
    "            node, community_id = map(int, line.strip().split()) # Renamed variable\n",
    "            ground_truth_dict[node] = community_id             # Use the new name\n",
    "            \n",
    "    print(f\"Graph: email-Eu-core\")\n",
    "    print(f\"Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}\")\n",
    "    \n",
    "    # --- RUN ALGORITHM AND GET RESULTS ---\n",
    "    print(\"\\n--- Running OBPSO-AIW with Full Evaluation ---\")\n",
    "    \n",
    "    # Note: This is a larger graph. The process will take longer.\n",
    "    # You might want to start with fewer iterations (e.g., max_iter=50) for a quick test.\n",
    "    obpso = OBPSO_AIW(G, population_size=100, max_iter=100, ground_truth=ground_truth_dict)\n",
    "    \n",
    "    best_partition_labels, all_results = obpso.run()\n",
    "\n",
    "    # --- PRINT FINAL RESULTS ---\n",
    "    print(\"\\n--- ✅ Final Results for email-Eu-core ---\")\n",
    "    for metric, value in all_results.items():\n",
    "        if isinstance(value, (float, np.floating)):\n",
    "            print(f\"{metric:<25}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric:<25}: {value}\")\n",
    "            \n",
    "    # --- STABILITY CALCULATION (Optional) ---\n",
    "    #This is very computationally intensive on a larger graph.\n",
    "     #Consider running with fewer runs and iterations or skipping it initially.\n",
    "    \n",
    "    print(\"\\n--- Starting Stability Calculation (this may take a while) ---\")\n",
    "    stability_score = calculate_stability(\n",
    "         G, \n",
    "         OBPSO_AIW, \n",
    "         runs=3, # Reduced runs for speed\n",
    "         population_size=50, \n",
    "         max_iter=50, \n",
    "        ground_truth=ground_truth_dict\n",
    "     )\n",
    "    print(f\"{'Stability (Avg. NMI)':<25}: {stability_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b8eeb-bb3f-4335-bc70-21278257f6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
